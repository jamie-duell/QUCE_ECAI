{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.autograd as autograd\n",
    "import saliency.core as saliency\n",
    "\n",
    "class ExplainerBase(object):\n",
    "\n",
    "    def __init__(self, model_interface, data_interface):\n",
    "\n",
    "        self.model_interface = model_interface\n",
    "        self.data_interface = data_interface\n",
    "\n",
    "    def generate_counterfactuals(self):\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def opposing_class_constraint(self):\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim * 2)  # Output has both mean and log variance\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_params = self.encoder(x)\n",
    "        z_params = z_params.view(-1, 2, self.latent_dim)  # Reshape to (batch_size, 2, latent_dim)\n",
    "        mu = z_params[:, 0, :]\n",
    "        logvar = z_params[:, 1, :]\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_x = self.decoder(z)\n",
    "        return reconstructed_x, mu, logvar\n",
    "\n",
    "\n",
    "        \n",
    "class QUCE(ExplainerBase):\n",
    "    def __init__(self, data_interface, model_interface, input_dim, latent_dim):\n",
    "        super().__init__(data_interface, model_interface)\n",
    "        self.vae = VAE(input_dim, latent_dim)\n",
    "        self.vae_optimizer = optim.Adam(self.vae.parameters(), lr=0.1)\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def compute_integrated_gradients_uncertainty(self, instances_list, num_steps=1000):\n",
    "        integrated_gradients = torch.zeros_like(torch.tensor(instances_list[0]))  # Convert to PyTorch tensor\n",
    "\n",
    "        start_instance_tensor = torch.tensor(instances_list[0], dtype=torch.float32, requires_grad=True)\n",
    "        end_instance_tensor = torch.tensor(instances_list[1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "            # Linear interpolation between start and end instances\n",
    "        path_interp = torch.linspace(0, 1, num_steps).view(-1, 1)\n",
    "        interp_instances = (1 - path_interp) * start_instance_tensor + path_interp * end_instance_tensor\n",
    "\n",
    "            # Compute model output at each interpolated point\n",
    "        outputs = self.model_interface(interp_instances)\n",
    "\n",
    "            # Sum the outputs to make it a scalar (assuming the model returns a tensor)\n",
    "        outputs_sum = torch.sum(outputs)\n",
    "\n",
    "            # Compute gradients at each interpolated point\n",
    "        gradients = torch.autograd.grad(outputs_sum, interp_instances, retain_graph=True)[0]\n",
    "\n",
    "            # Clip gradients to a reasonable range\n",
    "            #gradients = torch.clamp(gradients, min=-1.0, max=1.0)\n",
    "            # Integrate gradients along the path\n",
    "        integrated_gradients = torch.sum(gradients, dim=0)/len(gradients)*(end_instance_tensor.detach().numpy() - start_instance_tensor.detach().numpy())\n",
    "        return integrated_gradients\n",
    "    \n",
    "    \n",
    "    def compute_integrated_gradients_interpolation(self, instances_list, num_steps=100):\n",
    "        integrated_gradients = torch.zeros_like(torch.tensor(instances_list[0]))  # Convert to PyTorch tensor\n",
    "\n",
    "        for i in range(len(instances_list) - 1):\n",
    "            start_instance_tensor = torch.tensor(instances_list[i], dtype=torch.float32, requires_grad=True)\n",
    "            end_instance_tensor = torch.tensor(instances_list[i + 1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "            # Linear interpolation between start and end instances\n",
    "            path_interp = torch.linspace(0, 1, num_steps).view(-1, 1)\n",
    "            interp_instances = (1 - path_interp) * start_instance_tensor + path_interp * end_instance_tensor\n",
    "\n",
    "            # Compute model output at each interpolated point\n",
    "            outputs = self.model_interface(interp_instances)\n",
    "\n",
    "            # Sum the outputs to make it a scalar (assuming the model returns a tensor)\n",
    "            outputs_sum = torch.sum(outputs)\n",
    "\n",
    "            # Compute gradients at each interpolated point\n",
    "            gradients = torch.autograd.grad(outputs_sum, interp_instances, retain_graph=True)[0]\n",
    "\n",
    "            # Clip gradients to a reasonable range\n",
    "            #gradients = torch.clamp(gradients, min=-1.0, max=1.0)\n",
    "            # Integrate gradients along the path\n",
    "            integrated_gradients += torch.sum(gradients, dim=0)/len(gradients)*(end_instance_tensor - start_instance_tensor)\n",
    "\n",
    "        return integrated_gradients\n",
    "    \n",
    "    def compute_blur_integrated_gradients(self, instances_list, num_steps=100, blur_sigma=0.15):\n",
    "        integrated_gradients = torch.zeros_like(torch.tensor(instances_list[0]))  # Convert to PyTorch tensor\n",
    "\n",
    "        for i in range(len(instances_list) - 1):\n",
    "            start_instance_tensor = torch.tensor(instances_list[i], dtype=torch.float32, requires_grad=True)\n",
    "            end_instance_tensor = torch.tensor(instances_list[i + 1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "            # Linear interpolation between start and end instances\n",
    "            path_interp = torch.linspace(0, 1, num_steps).view(-1, 1)\n",
    "            interp_instances = (1 - path_interp) * start_instance_tensor + path_interp * end_instance_tensor\n",
    "\n",
    "            # Compute model output at each interpolated point\n",
    "            outputs = self.model_interface(interp_instances)\n",
    "\n",
    "            # Sum the outputs to make it a scalar (assuming the model returns a tensor)\n",
    "            outputs_sum = torch.sum(outputs)\n",
    "\n",
    "            # Compute gradients at each interpolated point\n",
    "            gradients = torch.autograd.grad(outputs_sum, interp_instances, retain_graph=True)[0]\n",
    "\n",
    "            # Apply blur to gradients\n",
    "            blurred_gradients = self.apply_blur(gradients, sigma=blur_sigma)\n",
    "\n",
    "            # Integrate blurred gradients along the path\n",
    "            integrated_gradients += torch.sum(blurred_gradients, dim=0) / len(blurred_gradients) * (\n",
    "                    end_instance_tensor - start_instance_tensor)\n",
    "\n",
    "        return integrated_gradients\n",
    "    \n",
    "    def apply_blur(gradients, sigma=0.15):\n",
    "        # Apply blur to gradients\n",
    "        blurred_gradients = torch.nn.functional.gaussian_blur(gradients, kernel_size=1, sigma=sigma)\n",
    "\n",
    "        return blurred_gradients\n",
    "    def generate_counterfactuals(self, query_instance, feature_mask=None ,time_constant_index=None, time_constant_diff=None, ts_dist_weight=0.5, reconstruction_weight=1, proba_weight=1, optimizer=None, target_prob_threshold=None, lr=0.01, max_iter=1000):\n",
    "        query_instance = torch.FloatTensor(query_instance)\n",
    "        num_features = len(query_instance)\n",
    "        cf_initialize = query_instance.clone().detach()  # Start with the query instance\n",
    "        cf_instances_list = []\n",
    "        integrated_gradients = torch.zeros_like(cf_initialize)\n",
    "        \n",
    "        if feature_mask is None:\n",
    "            feature_mask = torch.ones(num_features)\n",
    "        else:\n",
    "            feature_mask = torch.FloatTensor(feature_mask)\n",
    "\n",
    "        if optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam([cf_initialize], lr, betas=(0.9, 0.999))  # Adjust beta1 and beta2 as needed\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD([cf_initialize], lr, momentum=0.9)  # Adjust the momentum parameter as needed\n",
    "        \n",
    "        if target_prob_threshold == None:\n",
    "            if self.model_interface(query_instance) >= 0.5: \n",
    "                target_prob_threshold = 0.05  # Set the desired threshold\n",
    "            else:\n",
    "                target_prob_threshold = 0.95\n",
    "         \n",
    "        target_class_prob = torch.FloatTensor([target_prob_threshold])\n",
    "\n",
    "        self.vae.eval()\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            cf_initialize.requires_grad = True\n",
    "            optimizer.zero_grad()\n",
    "            cf_prob = self.model_interface(cf_initialize)\n",
    "    \n",
    "    # Pass cf_initialize through the VAE to get reconstructed_x, mu, and logvar\n",
    "            reconstructed_x, mu, logvar = self.vae(cf_initialize)\n",
    "            \n",
    "    # Calculate the loss\n",
    "            vae_reconstruction_loss = self.vae_loss(query_instance, reconstructed_x, mu, logvar)\n",
    "            loss = self.opposing_class_constraint(cf_prob, target_class_prob, query_instance\n",
    "                                                  , ts_dist_weight, proba_weight, cf_initialize) + reconstruction_weight*vae_reconstruction_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            cf_instances_list.append(cf_initialize.clone().detach().numpy())\n",
    "            # Convert cf_instances_list to PyTorch tensor\n",
    "        # Compute the absolute difference vector\n",
    "        diff_vector = reconstructed_x\n",
    "\n",
    "    # Run integrated gradients for perturbed_plus\n",
    "        perturbed_plus = cf_initialize + torch.abs(diff_vector)\n",
    "        explanation_plus = self.compute_integrated_gradients_uncertainty([cf_initialize, perturbed_plus])\n",
    "\n",
    "    # Run integrated gradients for perturbed_minus\n",
    "        perturbed_minus = cf_initialize - torch.abs(diff_vector)\n",
    "        explanation_minus = self.compute_integrated_gradients_uncertainty([cf_initialize, perturbed_minus])\n",
    "\n",
    "        integrated_gradients = self.compute_integrated_gradients_interpolation(cf_instances_list)\n",
    "        return cf_initialize, integrated_gradients, explanation_plus, explanation_minus, cf_instances_list\n",
    "    \n",
    "    \n",
    "    def exepcted_attribution(self, query_instance, p=5, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform iterative counterfactual generation and calculate the mean feature attribution.\n",
    "\n",
    "        Parameters:\n",
    "        - query_instance: The input instance for which counterfactuals are generated.\n",
    "        - p: The number of iterations.\n",
    "        - **kwargs: Additional arguments to pass to the generate_counterfactuals method.\n",
    "\n",
    "        Returns:\n",
    "        - mean_attributions: The mean feature attributions over p iterations.\n",
    "        \"\"\"\n",
    "        attributions_sum = torch.zeros_like(torch.tensor(query_instance), dtype=torch.float32)\n",
    "\n",
    "        for _ in range(p):\n",
    "            _, attributions, _, _, _ = self.generate_counterfactuals(query_instance, optimizer='adam', **kwargs)\n",
    "            attributions_sum += attributions\n",
    "\n",
    "        mean_attributions = attributions_sum / p\n",
    "\n",
    "        return mean_attributions\n",
    "    \n",
    "    def plot_explanations(self, integrated_gradients, explanation_plus, explanation_minus, feature_names=None):\n",
    "        num_features = len(integrated_gradients)\n",
    "        feature_indices = np.arange(num_features)\n",
    "\n",
    "    # Plot main integrated gradients\n",
    "        plt.bar(feature_indices, integrated_gradients.detach().numpy(), label='Integrated Path Attribution', color='grey', alpha=1)\n",
    "        center_positions = feature_indices + integrated_gradients.detach().numpy()\n",
    "    # Plot uncertainty bars\n",
    "        plt.bar(\n",
    "            center_positions,\n",
    "            explanation_plus.detach().numpy(),\n",
    "            bottom=integrated_gradients.detach().numpy(),\n",
    "            color='blue',\n",
    "            label='Uncertainty (+ε)',\n",
    "            alpha=1\n",
    "        )\n",
    "        plt.bar(\n",
    "            center_positions,\n",
    "            explanation_minus.detach().numpy(),\n",
    "            bottom=integrated_gradients.detach().numpy(),\n",
    "            color='red',\n",
    "            label='Uncertainty (-ε)',\n",
    "            alpha=1\n",
    "        )\n",
    "        plt.xticks(ticks=feature_indices, labels=feature_names, rotation='vertical')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Attribution Value')\n",
    "        plt.title('Path-Based Gradients with Uncertainty (ε)')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def train_vae(self, query_instance, num_epochs):\n",
    "        self.vae.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            instance = torch.FloatTensor(query_instance)\n",
    "            reconstructed_x, mu, logvar = self.vae(instance)\n",
    "            loss = self.vae_loss(instance, reconstructed_x, mu, logvar)\n",
    "            self.vae_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.vae_optimizer.step()\n",
    "\n",
    "    def vae_loss(self, x, reconstructed_x, mu, logvar):\n",
    "        # Calculate the VAE loss (reconstruction loss + KL divergence)\n",
    "        reconstruction_loss = nn.MSELoss()(reconstructed_x, x)  # Change the loss function for non-binary data\n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return reconstruction_loss + kl_divergence\n",
    "    \n",
    "    def reconstruction_loss(self, x, reconstructed_x):\n",
    "        # Calculate the VAE loss (reconstruction loss + KL divergence)\n",
    "        reconstruction_loss = nn.MSELoss()(reconstructed_x, x)  # Change the loss function for non-binary data\n",
    "        #kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return reconstruction_loss \n",
    "\n",
    "    \n",
    "    def opposing_class_constraint(self, cf_prob, target_class_prob, query_instance, ts_dist_weight, proba_weight, cf_initialize):\n",
    "    # Calculate a loss to ensure that the generated instance belongs to the opposing class and; \n",
    "    # the distance between query instance and the counterfactual is minimal\n",
    "\n",
    "        cf_prob = F.sigmoid(cf_prob)\n",
    "\n",
    "        target_class_loss = F.mse_loss(cf_prob, target_class_prob)\n",
    "   \n",
    "        distances = np.sum(np.linalg.norm(query_instance.detach().numpy() - cf_initialize.detach().numpy(), axis=1))\n",
    "\n",
    "        # Define the distance decay rate - latter points in time have a greater weighting\n",
    "        #alpha = 0.1 + 0.1 * np.arange(len(query_instances.detach().numpy()))\n",
    "\n",
    "        # Apply the exponential kernel to assign weights\n",
    "        #weights = np.exp(-alpha * distances)\n",
    "\n",
    "        # Normalize the weights (optional)\n",
    "        #normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "        # Compute the weighted result\n",
    "        #weighted_distance_list = np.dot(normalized_weights, query_instances.detach().numpy())\n",
    "        #weighted_distance = np.sum(weighted_distance_list)\n",
    "        return proba_weight*target_class_loss + ts_dist_weight*distances\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
